---
title: "Uniciclo con EKF in Python"
author: "Tommaso Faraci"
format:
  revealjs:
    incremental: true   
---

## Panoramica

- Modello del robot
- Controllore Proporzionale
- Modello dell'uniciclo in Python
- Sensori
- Filtro EKF
- Controllo con tastiera
- Visualizzazione

# Modello del robot

## Modello del robot

- Definiamo il vettore di stato: 
$$
\mathbf{x} = \begin{bmatrix} x \\ y \\ \theta \end{bmatrix}
$$

- Partiamo dal modello cinematico: 
$$
\begin{align*}
\dot{x} & = v \cos(\theta) \\
\dot{y} & = v \sin(\theta) \\
\dot{\theta} & = \omega
\end{align*}
$$

## Modello del robot


- Aggiungiamo la dinamica della velocità lineare come: 
$$
\dot{v} = a
$$
dove $$ a $$ è l'accelerazione lineare in avanti.

## Modello del robot
- La dinamica completa diventa:
$$
\begin{align*}
\dot{x} & = v \cos(\theta) \\
\dot{y} & = v \sin(\theta) \\
\dot{\theta} & = \omega \\
\dot{v} & = a
\end{align*}
$$

- Possiamo definire il vettore degli input come: 
$$\mathbf{u} = \begin{bmatrix} a \\ \omega \end{bmatrix}$$

- La dinamica è un'equazione differenziale non lineare del tipo:
$$\dot{\mathbf{x}} = f(\mathbf{x}, \mathbf{u})$$


## Modello del robot

- Per implementare numericamente l'evoluzione dello stato, usiamo l'integrazione di Eulero in avanti:
$$
\begin{align*}
x_{t+1} & = x_t + \dot{x}_t \Delta t \\
y_{t+1} & = y_t + \dot{y}_t \Delta t \\
\theta_{t+1} & = \theta_t + \dot{\theta}_t \Delta t \\
v_{t+1} & = v_t + \dot{v}_t \Delta t
\end{align*}
$$

- In forma vettoriale: 
$$\mathbf{x}_{t+1} = \mathbf{x}_t + f(\mathbf{x}_t, \mathbf{u}_t) \Delta t$$

## Dynamica numerica in python

```
import numpy as np
import matplotlib.pyplot as plt

dt = 0.1  # passo di integrazione
x = np.zeros((4, 1))  # stato iniziale [x, y, theta, v]
u = np.array([[1.0], [0.1]])  # input [a, omega]

def f(x, u):
    # Funzione di stato
    dot_x = x[3,0] * np.cos(x[2,0])
    dot_y = x[3,0] * np.sin(x[2,0])
    dot_v = u[0,0]
    dot_theta = u[1,0]
    return np.array([[dot_x], [dot_y], [dot_theta], [dot_v]])

# Simulazione
x_log = []
for t in range(100):
    x += f(x, u) * dt
    x_log.append(x.copy())

# Visualizzazione del percorso
x_log = np.array(x_log).squeeze()
plt.plot(x_log[:,0], x_log[:,1])
plt.xlabel('X position')
plt.ylabel('Y position')
plt.title('Percorso del robot uniciclo')
plt.axis('equal')
plt.grid()
plt.show()
print("Stato finale:", x.flatten())
```
```{python}
import numpy as np
import matplotlib.pyplot as plt

dt = 0.1  # passo di integrazione
x = np.zeros((4, 1))  # stato iniziale [x, y, theta, v]
u = np.array([[1.0], [0.1]])  # input [a, omega]

def f(x, u):
    # Funzione di stato
    dot_x = x[3,0] * np.cos(x[2,0])
    dot_y = x[3,0] * np.sin(x[2,0])
    dot_v = u[0,0]
    dot_theta = u[1,0]
    return np.array([[dot_x], [dot_y], [dot_theta], [dot_v]])

# Simulazione
x_log = []
for t in range(100):
    x += f(x, u) * dt
    x_log.append(x.copy())

# Visualizzazione del percorso
x_log = np.array(x_log).squeeze()
plt.plot(x_log[:,0], x_log[:,1])
plt.xlabel('X position')
plt.ylabel('Y position')
plt.title('Percorso del robot uniciclo')
plt.axis('equal')
plt.grid()
plt.show()
print("Stato finale:", x.flatten())
```

# Controllore Proporzionale

## Controllore Proporzionale

- Controllare l'accelerazione lineare e la velocità angolare da tastiera risulterebbe scomodo. 
- Per semplicità, vogliamo invece controllare la velocità lineare e l'angolo di orientamento.
- Introduciamo il vettore di comandi desiderati:
$$\mathbf{u}_{cmd} = \begin{bmatrix} v_{cmd} \\ \theta_{cmd} \end{bmatrix}$$

## Controllore Proporzionale

- Definiamo l'errore tra i comandi desiderati e lo stato attuale:
$$\mathbf{e} = \begin{bmatrix} v_{cmd} - v \\ \theta_{cmd} - \theta \end{bmatrix}$$
- Usiamo un controllore proporzionale per calcolare gli input effettivi della dinamica:
$$\mathbf{u} =  K_p^T \mathbf{e}$$
dove $$K_p = \begin{bmatrix} K_{p,v} & K_{p,\theta} \end{bmatrix}$$ è il vettore di guadagni del controllore proporzionale.

## Controllore Proporzionale in python

```python
import numpy as np
import matplotlib.pyplot as plt

dt = 0.1  # passo di integrazione
K_p = np.array([0.5, 2.0], dtype=float) #guadagni proporzionali per v e theta

theta_cmd = np.pi / 4  # comando di orientamento desiderato
v_cmd = 1.0            # comando di velocità lineare desiderato

u_cmd = np.array([v_cmd, theta_cmd], dtype=float)

state = np.array([0.0, 0.0, 0.0, 0.0], dtype=float)  # stato iniziale [x, y, theta, v]

def dynamics(state, u):

    x, y, v, theta = state

    a, omega = u

    dx = v * np.cos(theta)
    dy = v * np.sin(theta)
    dv = a
    dtheta = omega

    return np.array([dx, dy, dv, dtheta], dtype=float)

def control(state, u_cmd, K_p):
    x, y, v, theta = state

    e_v = u_cmd[0] - v
    e_theta = u_cmd[1] - theta

    a = K_p[0] * e_v
    omega = K_p[1] * e_theta

    return np.array([a, omega], dtype=float)


# Simulazione
state_log = []
input_log = []
times = np.arange(0, 10, dt)
for t in range(len(times)):
    u = control(state, u_cmd, K_p)
    state += dynamics(state, u) * dt
    state_log.append(state.copy())
    input_log.append(u.copy())


# Visualizzazione del percorso
state_log = np.array(state_log).squeeze()
input_log = np.array(input_log).squeeze()
plt.plot(times, state_log[:,2], label='Velocità lineare (m/s)')
plt.plot(times, input_log[:,0], label='Accelerazione lineare (m/s²)')
plt.axhline(v_cmd, xmin=0, xmax=10, ls='--', color='r', label='v_cmd')
plt.legend()
plt.xlabel('Tempo (s)')
plt.ylabel('Velocità e accelerazione lineare (m/s)')
plt.title('Performance del controllore proporzionale')
plt.axis('equal')
plt.grid()
plt.show()
```

```{python}
import numpy as np
import matplotlib.pyplot as plt

dt = 0.1  # passo di integrazione
K_p = np.array([0.5, 2.0], dtype=float) #guadagni proporzionali per v e theta

theta_cmd = np.pi / 4  # comando di orientamento desiderato
v_cmd = 1.0            # comando di velocità lineare desiderato

u_cmd = np.array([v_cmd, theta_cmd], dtype=float)

state = np.array([0.0, 0.0, 0.0, 0.0], dtype=float)  # stato iniziale [x, y, theta, v]

def dynamics(state, u):
   
    x, y, v, theta = state

    a, omega = u

    dx = v * np.cos(theta)
    dy = v * np.sin(theta)
    dv = a
    dtheta = omega

    return np.array([dx, dy, dv, dtheta], dtype=float)

def control(state, u_cmd, K_p):
    x, y, v, theta = state

    e_v = u_cmd[0] - v
    e_theta = u_cmd[1] - theta

    a = K_p[0] * e_v
    omega = K_p[1] * e_theta

    return np.array([a, omega], dtype=float)


# Simulazione
state_log = []
input_log = []
times = np.arange(0, 10, dt)
for t in range(len(times)):
    u = control(state, u_cmd, K_p)
    state += dynamics(state, u) * dt
    state_log.append(state.copy())
    input_log.append(u.copy())


# Visualizzazione del percorso
state_log = np.array(state_log).squeeze()
input_log = np.array(input_log).squeeze()
plt.plot(times, state_log[:,2], label='Velocità lineare (m/s)')
plt.plot(times, input_log[:,0], label='Accelerazione lineare (m/s²)')
plt.axhline(v_cmd, xmin=0, xmax=10, ls='--', color='r', label='v_cmd')
plt.legend()
plt.xlabel('Tempo (s)')
plt.ylabel('Velocità e accelerazione lineare (m/s)')
plt.title('Performance del controllore proporzionale')
plt.axis('equal')
plt.grid()
plt.show()
```

# Modello dell'uniciclo in Python

## Struttura della classe

- Definiamo una classe `Unicycle` per rappresentare il modello dell'uniciclo con dinamica e controllore proporzionale integrati.

    ```python
    class Unicycle:
    ```


- Inizializziamo attributi per lo stato, i comandi desiderati, i guadagni del controllore e la discretizzazione temporale.
   
    ```python
    class Unicycle:
        def __init__(self, x=0.0, y=0.0, v=0.0, theta=0.0, dt=0.02):
            self.state = np.array([x, y, v, theta], float)
            
            self.dt = dt
            self.a = 0.0
            self.v = v
            self.omega = 0.0

            self.v_cmd = 0.0      # linear acceleration command
            self.theta_cmd = 0.0   # angular velocity command

            self.u = np.array([self.a, self.omega], dtype=float)

            self.states_log = [self.state.copy()]

            self.K_p = np.array([0.5, 2.0], dtype=float)  # Proportional gains for v and theta
    ```

## Struttura della classe

- Implementiamo due metodi principali: `dynamics` per calcolare la dinamica del sistema e `step` che aggiorna lo stato in base agli input calcolati dal controllore proporzionale.

    ```python
        def dynamics(self, state, u):

        # Higher order dynamics for control inputs
        x, y, v, theta = state
        a, omega = u

        dx = v * np.cos(theta)
        dy = v * np.sin(theta)
        dv = a
        dtheta = omega

        return np.array([dx, dy, dv, dtheta], dtype=float)
    ```


## Struttura della classe

- ```python
def step(self, delta_t= None):

    u_cmd = np.array([self.v_cmd, self.theta_cmd], dtype=float)
    v = self.state[2]
    theta = self.state[3]

    error = u_cmd - np.array([v, theta], dtype=float)
    error[1] = (error[1] + np.pi) % (2 * np.pi) - np.pi  # wrap angle error to [-pi, pi]

    self.u = self.K_p * error # Acceleration and angular velocity commands are obtained proportionally

    dstate = self.dynamics(self.state, self.u)
    if delta_t is None:
        delta_t = self.dt

    self.state += dstate * delta_t
    self.state[3] = (self.state[3] + np.pi) % (2 * np.pi) - np.pi  # wrap theta to [-pi, pi]
    self.states_log.append(self.state.copy())
    ```

## Esempio di utilizzo

- La classe viene instanziata: 

    ```python
    uni = Unicycle()
    ```
- Modificando gli attributi `v_cmd` e `theta_cmd` possiamo impostare i comandi desiderati: 

    ```python
    uni.v_cmd = 1.0  # comando di velocità lineare desiderato
    uni.theta_cmd = np.pi / 4  # comando di orientamento desiderato
    ```

- Il metodo `step` viene chiamato in un ciclo per simulare il movimento del robot uniciclo:

    ```python
    for t in range(500):
        uni.v_cmd = 1.0  # comando di velocità lineare desiderato
        uni.theta_cmd = np.pi / 4  # comando di orientamento desiderato
        uni.step()
    ```

- Avendo definito l'attributo `states_log`, possiamo accedere allo storico degli stati del robot:

    ```python
    states = uni.states_log
    ```

<!-- 






SENSORI




 -->


# Sensori

## IMU 2D 
- Misura l'accelerazione lineare e la velocità angolare nel piano 2D.

- Modello di misura:
$$
\begin{align*}
a_{meas} & = a + b_{a} + n_{a} \\
\omega_{meas} & = \omega + b_{\omega} + n_{\omega} 
\end{align*}
$$

- L'accelerazione e la velocità angolare misurate sono affette da bias e rumore bianco gaussiano.

- Le misure sono espresse nel frame del robot, che indichiamo con B (Body frame).

## Dinamica del bias
- L'evoluzione dei bias è modellata come processi di passeggiata aleatoria (random walk):
$$
\begin{align*}
\dot{b}_{a} & = n_{ba} \\
\dot{b}_{\omega} & = n_{b\omega}
\end{align*}
$$

- Dove modelliamo i termini come rumori bianchi gaussiani che rappresentano la variazione casuale dei bias nel tempo.

## IMU 2D in python

- Definiamo una classe `IMU`:

    ```python
    class IMU:
        def __init__(self, dt,
                    gyro_bias=0.001, accel_bias=(0.005, -0.003),
                    gyro_noise_std=0.002, accel_noise_std=0.002,
                    gyro_bias_rw_std=0.0005, accel_bias_rw_std=0.0002,
                    seed=0):
            self.dt = dt # time step
        self.rng = np.random.default_rng(seed) # random number generator for reproducibility

        self.bg = float(gyro_bias) # Initial gyroscope bias
        self.ba = np.array(accel_bias, dtype=float) # Initial accelerometer bias


        self.gyro_noise_std = float(gyro_noise_std) # Std deviation of gyroscope noise
        self.accel_noise_std = float(accel_noise_std) # Std deviation of accelerometer noise

        self.gyro_bias_rw_std = float(gyro_bias_rw_std) # Std deviation of gyro bias random walk
        self.accel_bias_rw_std = float(accel_bias_rw_std) # Std deviation of acceleration bias random walk

        self.log = []        # log of measurements     
    ```
- Introduciamo come parametri i valori iniziali di bias e le deviazioni standard del rumore e della passeggiata aleatoria.

## IMU 2D in python

- Le misure sono modellate nel metodo `step`:

    ```python
    def step(self, state, u, delta_t=None):        
    ```
- Il metodo accetta lo stato attuale del robot, gli input di controllo e un intervallo di tempo opzionale.

- Per prima cosa, i bias vengono aggiornati con integrazione di Eulero in avanti:

    ```python
        # bias random walk update
        self.bg +=  delta_t * self.rng.normal(scale=self.gyro_bias_rw_std)
        self.ba +=  delta_t * self.rng.normal(scale=self.accel_bias_rw_std, size=2)
    ```

## IMU 2D in python

- Successivamente, calcoliamo l'accelerazione nel sistema fisso (World Frame), derivando la velocità lineare:

    $$
    \begin{align*}
    v_w & = v \cdot \begin{bmatrix} \cos(\theta) \\ \sin(\theta) \end{bmatrix} \\
    a_w & = \frac{d v_w}{dt} = \frac{d v}{dt} \begin{bmatrix} \cos(\theta) \\ \sin(\theta) \end{bmatrix} + v \, \frac{d \theta}{dt} \begin{bmatrix} -\sin(\theta) \\ \cos(\theta) \end{bmatrix} \\
    \end{align*}
    $$

- Il risultato è: 

    $$ 
    a_w = a \begin{bmatrix} \cos(\theta) \\ \sin(\theta) \end{bmatrix} + v \, \omega \begin{bmatrix} -\sin(\theta) \\ \cos(\theta) \end{bmatrix}
    $$

## IMU 2D in python

- In python: 

    ```python
        # world acceleration
        a_w = a * np.array([np.cos(th), np.sin(th)]) + omega * v * np.array([-np.sin(th), np.cos(th)])
    ```

- Le misure dell'IMU sono ottenute nel frame del robot (Body Frame) tramite la rotazione inversa, usando la funzione `rot2` per ottenere la matrice di rotazione 2D a partire dall'angolo di orientamento `th`:

    ```python
        # rotate to body frame
        R = rot2(th)
        ab = R.T @ a_w
    ```
    Il termine `R.T` rappresenta la trasposizione della matrice di rotazione, che corrisponde alla rotazione inversa. Mentre `@` indica la moltiplicazione matrice-vettore in numpy.

## IMU 2D in python

- Finalmente, possiamo implementare il modello di misura: 

    ```python
        # measurements with bias + noise
        gyro_z = (omega + self.bg) + self.rng.normal(scale=self.gyro_noise_std)
        accel_b = (ab + self.ba) + self.rng.normal(scale=self.accel_noise_std, size=2)
    ```

- Le misure sono salvate in un vettore `meas` e aggiunte al log delle misure:

    ```python
        meas = np.array([accel_b[0], accel_b[1], gyro_z]).reshape((3,1))
        self.log.append(meas)
        return meas
    ```

- La classe viene utilizzata instanziandola e chiamando il metodo `step` in un ciclo di simulazione.

    ```python
    imu = IMU(dt=0.02)
    for t in range(500):
        meas = imu.step(uni.state, uni.u)
    ```


## Camera 2D

- La camera simula la misura di landmark fissi e noti nell'ambiente.

- Assumiamo che guardi sempre in avanti lungo l'asse x del robot.

- Il modello di misura è quello della camera stenopeica 2D (Pinhole camera model).

- I parametri della camera sono la distanza focale `f` e il centro ottico `c`.

## Camera 2D 

- La camera misura la proiezione dei landmark nel frame del robot (Body Frame).

- Dato un landmark: 
$$
\mathbf{l}_w = \begin{bmatrix} l_{w,x} \\ l_{w,y} \end{bmatrix}
$$

- Otteniamo un'"immagine" 1D contenente il landmark, espresso come un pixel sull'asse orizzontale `u`.

## Camera 2D 

- Per calcolare la proiezione, prima trasformiamo il landmark dal frame fisso (World Frame) al frame del robot (Body Frame):
$$
\mathbf{l}_b = R_{B,W}^T (\mathbf{l}_w - \mathbf{p}_w)
$$

- Poi otteniamo le coordinate omogenee del landmark nel frame della camera, prima ottenendo la proiezione prospettica: 
$$
\tilde{\mathbf{l}}_b = \begin{bmatrix} \frac{l_{b,x}}{l_{b,x}} \\ \frac{l_{b,y}}{l_{b,x}} \end{bmatrix} = 
\begin{bmatrix} 1 \\ \frac{l_{b,y}}{l_{b,x}} \end{bmatrix}
$$

## Camera 2D 

- Infine, otteniamo la coordinata del pixel `u` come:
$$
z = \begin{bmatrix} c & f \end{bmatrix} \begin{bmatrix} 1 \\ \frac{l_{b,y}}{l_{b,x}} \end{bmatrix} 
= c + f \frac{l_{b,y}}{l_{b,x}}
$$

- Il modello di misura è affetto da rumore bianco gaussiano:
$$
z_{meas} = z + n_z
$$

## Verifica del campo visivo (FOV)

- Per verificare se un landmark è visibile dalla camera, controlliamo due condizioni:
    - Il landmark deve essere davanti alla camera: 
    $$ 
    l_{b,x} > 0
    $$
    - L'angolo tra l'asse della camera e la direzione verso il landmark deve essere entro il campo visivo definito dall'angolo di apertura `fov`:
    $$
    |\arctan(\frac{l_{b,y}}{l_{b,x}})| \leq \frac{fov}{2}
    $$

## Camera 2D in python

- Definiamo una classe `Camera`:

    ```python
    def __init__(self, intrinsics=np.array([0, 1]), dt=1.5):
        self.intrinsics = intrinsics  # Placeholder for camera intrinsics
        self.fov = np.pi / 4  # 45 degrees field of view
        self.min_range = 1.0
        self.max_range = 10.0
        self.noise = 0.001  # pixel noise standard deviation

        self.dt = dt
    ```

- Implementiamo i metodi `project` e `check_fov` per la proiezione dei landmark e la verifica del campo visivo.

    ```python
    def project(self, landmark_pos, robot_pos, R_BW)
        
    def check_fov(self, landmark_pos, robot_pos, robot_theta)
    ```

## Camera 2D in python

- Possiamo combinare il metodo `step` in un ciclo di simulazione insieme all'uniciclo per ottenere le misure dei landmark visibili:

    ```python
    
    uni = Unicycle()
    camera = Camera()


    landmarks = [np.array([5.0, 2.0]), np.array([8.0, -1.0]), np.array([3.0, 4.0])]

    uni.v_cmd = 1.0  # comando di velocità lineare desiderato
    uni.theta_cmd = np.pi / 4  # comando di orientamento desiderato
        
    for t in range(500):

        camera_measurements = camera.step(uni.state[0:2], uni.state[3], landmarks)

        uni.step()
    ```
        
## Sistema multi-sensore

- Combinando IMU e camera, otteniamo un sistema di sensori completo per l'uniciclo.

- La IMU fornisce misure rumorose che permettono di connettere due pose consecutive.

- La camera, tramite le posizioni note dei landmark, fornisce misure assolute che aiutano a correggere la deriva accumulata dall'IMU.

- In python: 

    ```python
    uni = Unicycle(dt=0.02)
    imu = IMU(dt=0.02)
    camera = Camera(dt=0.02)

    landmarks = [np.array([5.0, 2.0]), np.array([8.0, -1.0]), np.array([3.0, 4.0])]

    for t in range(500):
        imu_meas = imu.step(uni.state, uni.u)
        camera_measurements = camera.step(uni.state[0:2], uni.state[3], landmarks)
        uni.step()
    ```


# Filtro EKF

## Filtro EKF 
- Il filtro EKF combina le misure rumorose dell'IMU e della camera per stimare lo stato del robot uniciclo.

- La fase di predizione utilizza la dinamica dell'IMU per prevedere il nuovo stato e la sua covarianza.

- La fase di aggiornamento utilizza le misure della camera per correggere la stima dello stato e ridurre l'incertezza.

## Stato dell'EKF 

- Il nostro filtro stima lo stato dell'IMU ( e quindi del robot uniciclo) che include posizione, orientamento, velocità lineare e bias dei sensori:
$$
\mathbf{x} = \begin{bmatrix} x \\ y  \\ v_x \\ v_y \\ \theta \\ b_{a_x} \\ b_{a_y}  \end{bmatrix}
$$

## Input e modello dell'EKF
- Gli input al filtro EKF sono le misure dell'IMU:
$$
\mathbf{u} = \begin{bmatrix} a_{meas,x} \\ a_{meas,y} \\ \omega_{meas} \end{bmatrix}
$$

- Il modello dinamico è basato sulla dinamica dell'IMU descritta in precedenza: 
$$
\dot{\mathbf{x}} = f_{IMU}(\mathbf{x}, \mathbf{u}_t)
$$

## Rumore di processo
- Il vettore di rumore di processo è definito come: 
$$
\mathbf{n}_{IMU} = \begin{bmatrix} n_{a_x} \\ n_{a_y} \\ n_{\omega} \\ n_{b_{a_x}} \\ n_{b_{a_y}} \\ n_{b_{\omega}} \end{bmatrix}
$$

## Covarianze del rumore

- Vi si associa una matrice di covarianza dipendente dalla IMU stessa:
$$
\mathbf{Q}_{IMU} = diag (\sigma_{a_x}^2, \sigma_{a_y}^2, \sigma_{\omega}^2, \sigma_{b_{a_x}}^2, \sigma_{b_{a_y}}^2, \sigma_{b_{\omega}}^2)
$$

## Rumore di misura
- Il rumore di misura della camera è anch'esso modellato come rumore bianco gaussiano con covarianza:
$$
\sigma_{z}^2
$$

- Per ogni landmark misurato, viene introdotta una misura indipendente con lo stesso modello di rumore, ottenendo una matrice di covarianza complessiva:
$$
\mathbf{R} = diag (\sigma_{z_1}^2, \sigma_{z_2}^2, ..., \sigma_{z_n}^2)
$$

## Predizione

- Calcolo dello stato predetto:
    $$
    \dot{\hat{\mathbf{x}}}_{t|t} =  \hat{f}_{IMU}(\hat{\mathbf{x}}_{t|t}, \mathbf{u}_t)
    $$
    $$
    \hat{\mathbf{x}}_{t+1|t} = \hat{\mathbf{x}}_{t|t} + \hat{f}_{IMU}(\hat{\mathbf{x}}_{t|t}, \mathbf{u}_t) \cdot dt
    $$

- Propagazione covarianza: 

    $$
    \dot{\mathbf{P}}_t = \mathbf{F}_t \mathbf{P}_{t|t} \mathbf{F}_t^T +  \mathbf{G}_t \mathbf{Q}_{IMU,t} \mathbf{G}_t^T
    $$
    $$
    \mathbf{P}_{t+1|t} = \mathbf{P}_{t|t} + \dot{\mathbf{P}}_t \cdot dt
    $$

## Introduzione di misure

- Data una misura della camera, si aggiorna lo stato dell'EKF. 

- Calcola la predizione della misura:
    $$
    \tilde{\mathbf{z}}_{t+1|t} = h(\hat{\mathbf{x}}_{t+1|t})
    $$

- Covarianza dell'innovazione:
    $$
    \mathbf{S}_t = \mathbf{H}_t \mathbf{P}_{t+1|t} \mathbf{H}_t^T + \mathbf{R}_t
    $$

- Calcolo del guadagno di Kalman:
    $$
    \mathbf{K}_t = \mathbf{P}_{t+1|t} \mathbf{H}_t^T \mathbf{S}_t^{-1}
    $$

## Aggiornamento dello stato
- Aggiornamento di stato:
    $$
    \hat{\mathbf{x}}_{t+1|t+1} = \hat{\mathbf{x}}_{t+1|t} + \mathbf{K}_t (\mathbf{z}_t - \tilde{\mathbf{z}}_t)
    $$

- Aggiornamento della covarianza
    $$
    \mathbf{P}_{t+1|t +1} = ( \mathbf{I} - \mathbf{K}_t \mathbf{H}_t ) \mathbf{P}_{t+1|t} 
    $$

## Jacobiani 

- Trattandosi di sistemi non-lineari, è necessario calcolare i jacobiani delle funzioni di stato e di misura.

- In generale, questi sono ri-calcolati ad ogni passo del filtro EKF, utilizzando la stima corrente dello stato.

- Possono essere calcolati analiticamente oppure numericamente tramite differenze finite.

## Jacobiani

- I jacobiani sono definiti come:
    $$
    \mathbf{F} = \frac{\partial f_{IMU}}{\partial \mathbf{x}} \bigg|_{\hat{\mathbf{x}} , \quad \mathbf{u}}
    $$

    $$
    \mathbf{H} = \frac{\partial h}{\partial \mathbf{x}} \bigg|_{\hat{\mathbf{x}}}
    $$

    $$ 
    \mathbf{G} = \frac{\partial f_{IMU}}{\partial \mathbf{n}_{IMU}} \bigg|_{\hat{\mathbf{x}} , \quad \mathbf{u}}
    $$

## Jacobiano della dinamica
$$
\mathbf{F} = 
\begin{bmatrix}
\mathbf{0}_{2 \times 2} & \mathbf{I}_{2\times2} & \mathbf{0}_{2 \times 1} & \mathbf{0}_{2 \times 2} & \mathbf{0}_{2 \times 1} \\ 
% 
% 
\mathbf{0}_{2 \times 2} & \mathbf{0}_{2 \times 2} & \frac{d \mathbf{v}}{d \theta} & -\mathbf{R}_{WB} & \mathbf{0}_{2 \times 1} \\
\mathbf{0}_{1 \times 2} & \mathbf{0}_{1 \times 2} & 0 & \mathbf{0}_{1 \times 2} & -1 \\
\mathbf{0}_{2 \times 2} & \mathbf{0}_{2 \times 2} & \mathbf{0}_{2 \times 1} & \mathbf{0}_{2 \times 2} & \mathbf{0}_{2 \times 1} \\
\mathbf{0}_{1 \times 2} & \mathbf{0}_{1 \times 2} & 0 & \mathbf{0}_{1 \times 2} & 0
\end{bmatrix}
$$

dove 
$$
\frac{d \mathbf{v}}{d \theta} = 
\mathbf{R}_{WB} \begin{bmatrix}
 0 & -1 \\ 
    1 & 0
\end{bmatrix}  (\mathbf{a} - \mathbf{b}_a) = \mathbf{R}_{WB} 1{\hat{}} (\mathbf{a} - \mathbf{b}_a)
$$

## Jacobiano della dinamica in python
```python

self.F = np.zeros((8, 8))  # State transition Jacobian

self.F[0:2, 2:4] = np.eye(2)
temp  = R_WB @ hat2(1) @ (acc -b_a)
self.F[2:4, 4] = temp.flatten()  # dv/dtheta
self.F[2:4, 5:7] = -R_WB
self.F[4, -1] = -1  # dtheta/db_omega
```


## Jacobiano del rumore di processo

$$
\mathbf{G} =
\begin{bmatrix}
\mathbf{0}_{2 \times 2} & \mathbf{0}_{2 \times 1} & \mathbf{0}_{2 \times 2} & \mathbf{0}_{2 \times 1} \\
%
%
\mathbf{R}_{WB} & \mathbf{0}_{2 \times 1} & \mathbf{0}_{2 \times 2} & \mathbf{0}_{2 \times 1} \\
%
%
\mathbf{0}_{1 \times 2} & 1 & \mathbf{0}_{1 \times 2} & 0 \\
%
%
\mathbf{0}_{2 \times 2} & \mathbf{0}_{2 \times 1} & \mathbf{I}_{2 \times 2} & \mathbf{0}_{2 \times 1} \\
%
%
\mathbf{0}_{1 \times 2} & 0 & \mathbf{0}_{1 \times 2} & 1
\end{bmatrix}
= \begin{bmatrix}
\mathbf{0}_{2 \times 2} & \mathbf{0}_{2 \times 4} \\
\mathbf{R}_{WB} & \mathbf{0}_{2 \times 4} \\
\mathbf{0}_{4 \times 2} & \mathbf{I}_{4 \times 4} 
\end{bmatrix}
$$

## Jacobiano del rumore di processo in python
```python
self.G = np.zeros((8, 6)) # Process noise Jacobian


self.G[-4:, -4:] = np.eye(4)  # Process noise affects orientation
#                               and biases


self.G[2:4, 0:2] = R_WB  # Process noise affects velocity
```

## Jacobiani di processo in python
```python
def set_jacobians(self):

        """
        Set the Jacobian matrices for the EKF.
        """

        th = self.state[4, 0] # orientation angle
        acc = self.imu_measurement[0:2] # acceleration in body frame
        b_a = self.state[5:7] # accelerometer bias
        b_omega = self.state[7,0] # gyroscope bias

        R_WB = rot2(th)  # Rotation matrix from body to world frame

        self.F = np.zeros((8, 8))  # State transition Jacobian

        self.F[0:2, 2:4] = np.eye(2)
        temp  = R_WB @ hat2(1) @ (acc -b_a)
        self.F[2:4, 4] = temp.flatten()  # dv/dtheta
        self.F[2:4, 5:7] = -R_WB
        self.F[4, -1] = -1  # dtheta/db_omega

        self.G = np.zeros((8, 6)) # Process noise Jacobian
        

        self.G[-4:, -4:] = np.eye(4)  # Process noise affects biases and orientation
        self.G[2:4, 0:2] = R_WB  # Process noise affects velocity
```

## Jacobiano della misura
- Si calcola il jacobiano della funzione di misura della camera rispetto allo stato sfruttando la 'chain rule' delle derivate. 

- Possiamo scrivere la funzione di misura come:
$$
z= h(\mathbf{x}) = \mathbf{K} \tilde{\mathbf{l}}_b (\mathbf{l}_b)
$$

- Dove:
$$
\tilde{\mathbf{l}}_b (\mathbf{l}_b)   = \begin{bmatrix} 1 \\ \frac{l_{b,y}}{l_{b,x}} \end{bmatrix} 
$$

## Jacobiano della misura
- Ricordando che: 
$$
\mathbf{l}_b = \mathbf{R}_{WB}^T (\mathbf{l}_w - \mathbf{p}_w)
$$

- Scriviamo: 
$$
\frac{\partial z}{\partial \mathbf{x}} = \frac{\partial z}{\partial \tilde{\mathbf{l}}_b} \cdot \frac{\partial \tilde{\mathbf{l}}_b}{\partial \mathbf{l}_b} \cdot \frac{\partial \mathbf{l}_b}{\partial \mathbf{x}}
$$

## Jacobiano della misura

$$
\frac{\partial z}{\partial \tilde{\mathbf{l}}_b} = 
\frac{\partial}{\partial \tilde{\mathbf{l}}_b}(\mathbf{K} \tilde{\mathbf{l}}_b) =\mathbf{K} = \begin{bmatrix} c & f \end{bmatrix}
$$


$$ 
\frac{\partial \tilde{\mathbf{l}}_b}{\partial \mathbf{l}_b} = 
\begin{bmatrix}
0 & 0 \\
\frac{-l_{b,y}}{l_{b,x}^2} & \frac{1}{l_{b,x}}
\end{bmatrix}
$$

$$
\frac{\partial \mathbf{l}_b}{\partial \mathbf{x}} = \\
\begin{bmatrix}
 -\mathbf{R}_{WB}^T & \mathbf{0}_{2 \times 1} & - \mathbf{R}_{WB}^T \hat{1} (\mathbf{l}_w - \mathbf{p}_w) & \mathbf{0}_{2 \times 2} & \mathbf{0}_{2 \times 1}
\end{bmatrix}
$$


## Jacobiano della misura in python
```python
def measurement_jacobian(self, pred_state, landmark_w, landmark_b, R_BW, R_WB):
        """
        Compute the measurement Jacobian H.
        """

        H = np.zeros((1, 8))  # Adjust size based on measurement dimension

        du_dl_tilde_b = self.intrinsics # Derivative of pixel coordinates w.r.t. landmark homogenous coordinates in body frame
        
        dl_tilde_b_d_l_b = np.array([
            [0, 0], 
            [-landmark_b[1, 0]/(landmark_b[0, 0]**2), 1/landmark_b[0, 0]]
        ]) # Derivative of homogenous landmark coordinates in body frame w.r.t. landmark position in body frame

        dl_b_dtheta = -R_BW @ hat2(1) @ (landmark_w - pred_state[0:2]) # Derivative of landmark in body coordinates wrt orientation theta
        dl_b_d_p_w = -R_BW # Derivative of landmark in body coordinates wrt robot position in world coordinates


        dl_b_d_state = np.zeros((2, 8)) # Preallocate derivative of landmark in body coordinates wrt state

        dl_b_d_state[:, 0:2] = dl_b_d_p_w # Fill the matrix in the correct spots
        dl_b_d_state[:, 4] = dl_b_dtheta.reshape((2,))

        du_d_state = du_dl_tilde_b @ dl_tilde_b_d_l_b @ dl_b_d_state # Chain rule to get derivative of pixel coordinates wrt state


        return du_d_state
```

## EKF nella simulazione
- All'interno della funzione `update` della simulazione, integriamo i passi di predizione e aggiornamento dell'EKF:

    ```python
    # One frame means we run 1 IMU and 1 camera step

    # IMU measurement from current state, input and delta_t
    meas = self.imu.step(self.uni.state, self.uni.u, delta_t=self.delta_t) 

    # We use the measurement to predict the next state with the EKF
    self.ekf.predict(imu_measurement=meas)

    # Camera measurement from current state and landmarks
    camera_measurements = self.camera.step(self.landmarks, self.uni.state[0:2], self.uni.state[3]) 

    # Use the camera measurements to update the EKF, correcting the prediction
    kalman_estimate = self.ekf.update(camera_measurements, delta_t=self.delta_t)
    ```

# Ottenere il codice

Il codice è disponibile al seguente link:

https://github.com/idra-lab/slam-competition


# Visualizzazione dei risultati
- La simulazione utilizza Matplotlib per visualizzare il percorso del robot uniciclo, le stime dell'EKF e le posizioni dei landmark.

- La simulazione viene lanciata dalla cartella del workspace di ROS2 del progetto con il comando: 

    ```bash
    python3 src/slam-competition/scripts/unicycle_estimation.py
    ```

